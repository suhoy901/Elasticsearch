# Analysis 자세히

![](./asset/tuto5/analysis.png)



### Analyzer 의 구성요소

- **Character filters**
  - 원본 text 를 사전에 가공하는 과정
  - 설정하지 않거나 **다중으로 필터 설정**이 가능함
    - ex) html 태그 제거, 패턴매칭(123-456-789 -> 123_456_789), Hindu-Arabic 숫자 (٠١٢٣٤٥٦٧٨٩) 를 아라비아 숫자로 매핑 (0123456789)
- **Tokenizer**
  - 어떠한 방식으로 원본 text 를 토크나이징 할 것인가 결정
  - 토크나이징 된 term은 token이라고 함
  - **하나의 Tokenizer**만 설정이 가능함
    - ex) 공백을 기준으로 토크나이징 : You are a boy!!! -> You / are / a / boy!!!
- **Token filters**
  - Tokenizer에 의해 결정된 token 들을 가공
  - 서정하지 않거나 다중으로 필터 설정이 가능함
    - ex) stopwords 로 지정된 단어를 제거 :  You / are / a / boy -> you / boy





## 1. standard Analyzer

- "winter is Coming!!!" -> "winter" / "is" / "coming"

~~~
POST _analyze
{
  "text": "Winter is Coming!!!"
}


{
  "tokens" : [
    {
      "token" : "winter",
      "start_offset" : 0,
      "end_offset" : 6,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "is",
      "start_offset" : 7,
      "end_offset" : 9,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "coming",
      "start_offset" : 10,
      "end_offset" : 16,
      "type" : "<ALPHANUM>",
      "position" : 2
    }
  ]
}
~~~



### 1.1 filter로 lowercase, asciifolding을 적용

- "Is this déja vu?" -> "is" / "this" / "deja" / "vu"

~~~
POST _analyze
{
  "tokenizer": "standard",
  "filter": [
    "lowercase",
    "asciifolding"
  ],
  "text": "Is this déja vu?"
}


{
  "tokens" : [
    {
      "token" : "is",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "this",
      "start_offset" : 3,
      "end_offset" : 7,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "deja",
      "start_offset" : 8,
      "end_offset" : 12,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "vu",
      "start_offset" : 13,
      "end_offset" : 15,
      "type" : "<ALPHANUM>",
      "position" : 3
    }
  ]
}
~~~





## 2. whitespace Analyzer

- "winter is coming!!!" -> "Winter" / "is" / "coming!!!"

~~~
# whitespace
POST _analyze
{
  "analyzer": "whitespace",
  "text": "Winter is coming!!!"
}


{
  "tokens" : [
    {
      "token" : "Winter",
      "start_offset" : 0,
      "end_offset" : 6,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "is",
      "start_offset" : 7,
      "end_offset" : 9,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "coming!!!",
      "start_offset" : 10,
      "end_offset" : 19,
      "type" : "word",
      "position" : 2
    }
  ]
}
~~~





### 2.1 character filter : html_strip 사용, filter : uppercase

- \<B>This is mixed analyzer\</B> -> "THIS" / "IS" / "MIXED" / "ANALYZER"

~~~
POST _analyze
{
  "char_filter": [
    "html_strip"
  ],
  "tokenizer": "whitespace",
  "filter": [
    "uppercase"
  ],
  "text": "<B>This is mixed analyzer</B>"
}


{
  "tokens" : [
    {
      "token" : "THIS",
      "start_offset" : 3,
      "end_offset" : 7,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "IS",
      "start_offset" : 8,
      "end_offset" : 10,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "MIXED",
      "start_offset" : 11,
      "end_offset" : 16,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "ANALYZER",
      "start_offset" : 17,
      "end_offset" : 29,
      "type" : "word",
      "position" : 3
    }
  ]
}
~~~



## 3. English Analyzer

- "Winter is coming!!!" -> "winter" / "come" 

~~~
POST _analyze
{
  "analyzer": "english",
  "text": "Winter is coming!!!"
}


{
  "tokens" : [
    {
      "token" : "winter",
      "start_offset" : 0,
      "end_offset" : 6,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "come",
      "start_offset" : 10,
      "end_offset" : 16,
      "type" : "<ALPHANUM>",
      "position" : 2
    }
  ]
}
~~~



## 4. 이미 정의되어 있는 Analyzer를 사용하는 방식

### 4.1 이미 정의되어 있는 analyzer를 그대로 사용

- "\<B>This is Standard Analyzer\</B>"  -> "b" / "stand" / "ard" / "analy" / "zer" / "b"
  - standard
  - 길이 5
  - english stopwords

~~~
## 1. index정의시에 함께 설정
PUT index_analyzer_settings1
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "type": "standard",
          "max_token_length": 5,
          "stopwords": "_english_"
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "comment": {
        "type": "text",
        "analyzer": "my_analyzer"
      }
    }
  }
}


{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "index_analyzer_settings1"
}



## 2. analyze
POST index_analyzer_settings1/_analyze
{
  "analyzer": "my_analyzer",
  "text": "<B>This is Standard Analyzer</B>"
}


{
  "tokens" : [
    {
      "token" : "b",
      "start_offset" : 1,
      "end_offset" : 2,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "stand",
      "start_offset" : 11,
      "end_offset" : 16,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "ard",
      "start_offset" : 16,
      "end_offset" : 19,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "analy",
      "start_offset" : 20,
      "end_offset" : 25,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "zer",
      "start_offset" : 25,
      "end_offset" : 28,
      "type" : "<ALPHANUM>",
      "position" : 6
    },
    {
      "token" : "b",
      "start_offset" : 30,
      "end_offset" : 31,
      "type" : "<ALPHANUM>",
      "position" : 7
    }
  ]
}


## document 색인
POST index_analyzer_settings1/_doc
{
  "comment": "<B>This is Standard Analyzer</B>"
}


{
  "_index" : "index_analyzer_settings1",
  "_type" : "_doc",
  "_id" : "XQCRTHAB8UseQHot1xZ0",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}
~~~



### 4.2 이미 정의되어 있는 tokenizer에 character filter, token filter 를 조합하여 사용

- "\<B>This is Standard Analyzer\</B>" -> "THIS" / "IS" / "STANDARD" / "ANALYZER"
  - custom설정
  - character_filter : html_strip
  - tokenizer: standard
  - filter : uppercase

~~~
PUT index_analyzer_settings2
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "type": "custom",
          "char_filter": [
            "html_strip"
          ],
          "tokenizer": "standard",
          "filter": [
            "uppercase"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "comment": {
        "type": "text",
        "analyzer": "my_analyzer"
      }
    }
  }
}


{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "index_analyzer_settings2"
}



## 2. analyze
POST index_analyzer_settings2/_analyze
{
  "analyzer": "my_analyzer",
  "text": "<B>This is Standard Analyzer</B>"
}


{
  "tokens" : [
    {
      "token" : "THIS",
      "start_offset" : 3,
      "end_offset" : 7,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "IS",
      "start_offset" : 8,
      "end_offset" : 10,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "STANDARD",
      "start_offset" : 11,
      "end_offset" : 19,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "ANALYZER",
      "start_offset" : 20,
      "end_offset" : 32,
      "type" : "<ALPHANUM>",
      "position" : 3
    }
  ]
}


## 3. document 색인
POST index_analyzer_settings2/_doc
{
  "comment": "<B>This is Standard Analyzer</B>"
}


{
  "_index" : "index_analyzer_settings2",
  "_type" : "_doc",
  "_id" : "XgCSTHAB8UseQHotlxYE",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}
~~~



### 4.3 특정 필드를 매핑

- match query로 comment 필드에 "standard" 단어로 search

~~~
GET index_analyzer_settings2/_search
{
  "query": {
    "match": {
      "comment": "standard"
    }
  }
}


{
  "took" : 10,
  "timed_out" : false,
  "_shards" : {
    "total" : 3,
    "successful" : 3,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 0.2876821,
    "hits" : [
      {
        "_index" : "index_analyzer_settings2",
        "_type" : "_doc",
        "_id" : "XgCSTHAB8UseQHotlxYE",
        "_score" : 0.2876821,
        "_source" : {
          "comment" : "<B>This is Standard Analyzer</B>"
        }
      }
    ]
  }
}
~~~



### 4.4 Mixed analyzer

~~~
## 1. index 정의
PUT mixed_analyzer
{
  "settings": {
    "analysis": {
      "char_filter": {
        "my_char_filter": {
          "type": "mapping",
          "mappings": [
            ":) => _happy_",
            ":( => _sad_"
          ]
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "standard",
          "max_token_length": 20
        }
      },
      "filter": {
        "my_stop": {
          "type": "stop",
          "stopwords": [
            "and",
            "is",
            "the",
            "this"
          ]
        }
      },
      "analyzer": {
        "my_analyzer": {
          "type": "custom",
          "char_filter": [
            "html_strip",
            "my_char_filter"
          ],
          "tokenizer": "my_tokenizer",
          "filter": [
            "lowercase",
            "my_stop"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "comment": {
        "type": "text",
        "analyzer": "my_analyzer"
      }
    }
  }
}


{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "mixed_analyzer"
}



## 2. 정의한 analyzer로 analyze
POST mixed_analyzer/_analyze
{
  "analyzer": "my_analyzer",
  "text": "<B>This is My Analyzer :)</B>"
}


{
  "tokens" : [
    {
      "token" : "my",
      "start_offset" : 11,
      "end_offset" : 13,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "analyzer",
      "start_offset" : 14,
      "end_offset" : 22,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "_happy_",
      "start_offset" : 23,
      "end_offset" : 29,
      "type" : "<ALPHANUM>",
      "position" : 4
    }
  ]
}


## 3. document 색인
POST mixed_analyzer/_doc
{
  "comment": "<B>This is My Analyzer :)</B>"
}


{
  "_index" : "mixed_analyzer",
  "_type" : "_doc",
  "_id" : "ui6qTHABdFHMuraOXf7v",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}



## 4. my analyzer 검색어로 search
{
  "_index" : "mixed_analyzer",
  "_type" : "_doc",
  "_id" : "ui6qTHABdFHMuraOXf7v",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}


{
  "took" : 1029,
  "timed_out" : false,
  "_shards" : {
    "total" : 3,
    "successful" : 3,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 0.5753642,
    "hits" : [
      {
        "_index" : "mixed_analyzer",
        "_type" : "_doc",
        "_id" : "ui6qTHABdFHMuraOXf7v",
        "_score" : 0.5753642,
        "_source" : {
          "comment" : "<B>This is My Analyzer :)</B>"
        }
      }
    ]
  }
}
~~~



## 5. Nori Plugin

- 모든 노드에 노리 플러그인이 이미 설치되어 있어야함

~~~
PUT nori_sample
{
  "settings": {
    "analysis": {
      "tokenizer": {
        "nori_user_dict": {
          "type": "nori_tokenizer",
          "decompound_mode": "mixed",
          "user_dictionary": "userdict_ko.txt"
        }
      },
      "analyzer": {
        "my_analyzer": {
          "type": "custom",
          "tokenizer": "nori_user_dict"
        }
      }
    }
  }
}


{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "nori_sample"
}
~~~

