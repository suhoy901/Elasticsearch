# Airflow로 Elasticsearch Management

- https://airflow.readthedocs.io/en/latest/operators-and-hooks-ref.html

<br>

## ELK Stack

> **Distributed, Scalable**
>
> - `Cluster`,  `Node`, `index`, `Shard`, `Replica`
>
> **HA, Fault Tolerance**
>
> **Near Real-Time (NRT)**
>
> - `indexing`, `Refresh`
>
> **Full Text Search Engine**
>
> - `Document`, `Segment`, `Inverted Index`, `Token`
>
> **REST APIs**
>
> **JSON Document NoSQL**
>
> - `JSON`, `REST`
>
> **Multi-tenancy**
>
> **Distributed Operation & Aggregation**
>
> - `Aggregation`, `distributed`, `query`, `fetch`



<br>

<br>

# Kibana

> Elasticsearch에 대한 모니터링은 별도의 solution 혹은 다른 도구를 사용하는 것이 좋음
>
> - 단점 : alert기능이 유료....
> - ex) Grafana 등..
>
> tip) Kibana에서 csv파일을 로드하는 경우 100MB 이하의 데이터만 가능



<br>

## Kibana 에서의 데이터 처리 프로세스(csv 예시)

> 1. **CSV**
>
> 2. **Field확인**
>
>    - Time Field에 대해서 중요하게 생각함
>    - 그래서 새로운 timestamp라는 데이터타입으로 새로운 변수를 만들어냄
>
> 3. **Index를 만듦**
>
>    - Shard 개수를 지정
>
>    - mapping을 설정(Field에 대한 데이터 타입).
>
>    - Index pattern을 설정
>
> 4. **ES에 저장**
>
>    - Index에 데이터를 저장할 때 어떻게 할 것인지 Kibana가 물어보는 예시

![](../asset/kibanacsv_sample1.PNG)



<br>

## Index Management

> 로우 데이터는 : 15MB
>
> ES의 데이터 크기 : 31MB
>
> - 추가적인 메타정보 등의 것들이 들어가서 데이터의 크기가 커지는 것을 알 수 있음

![](../asset/data.PNG)



## Discovery

> **KQL(Kibana Query Language)**
>
> @Timestamp가 없으면 확인할 수 없음
>
> 필드정보를 선택하여 top 5개를 확인할 수 있음

**필터 기능을 많이 사용**해야함

- 되도록이면 Query를 많이 날리기 보다는 Filter기능을 많이 사용하는 것이 좋음



### Filter

> add filter로 변수를 선택하고 설정해주면 됨



### KSQL

> Kibana Query Language(KQL)
>
> - https://www.elastic.co/guide/en/kibana/7.6/kuery-query.html

~~~shell
# 자동완성 지원
category.keyword : "Men's Clothing" and customer_gender :"FEMALE" 
~~~

<br>

## Visualization & Dashboard

### Lens

> 데이터를 한번 탐색해보라고 사용하는 애
>
> - 필드들을 하나씩 옮겨서 쭉 확인해보는 것도 좋음
> - Buket Aggregation, Metric Aggregation 등의 aggregation을 직접 사용하지 않고 클릭으로 확인해볼 수 있음



<br>

## Index Management

> tip) Kibana에서 Elasticsearch로 색인된 데이터를 확인하고자 할 때 Pattern pattern을 설정해야 함

![](../asset/index_pattern.PNG)

<br>

## Monitoring

![](../asset/mornitoring.PNG)

![](../asset/mornitoring2.PNG)

- index rate : 색인 성능(데이터를 넣을 때)
- search rate : 검색 성능(데이터를 검색)
- 초당 25건이라는 의미(index rate)





# 3가지 관점의 Elasticsearch

> **Distributed Storage**
>
> **Distributed Search Engine**
>
> **Distributed Processing Engine**





<br>

# 1. DataType

> Elastic Sample Data
>
> - eCommerce
>   - 데이터 포맷, 대시보드
>
> - flight data
>
>   - Geo 포맷, 택시 위치 정보
>   - 누구를 매칭
>   - 가장 저렴한 가격
>
> - **Web logs**
>
>   - 로그 검색용으로 많이 사용했음(앞으로 만들어야할 애임)
>
>     - JSON 의 형태를 참고(30개 정도 있음)
>       - 필드에 맞는 데이터 타입을 정의(mapping)
>       - 매핑 참고
>



<br>

## Meta field

> - **_index**: 문서가 속한 index
> - **\_type**:_doc
>   - 삭제 예정. 사용 X
> - **\_id** : 문서 id
>   - id는 일반적으로 직접적으로 설정하지 않음
>   - 문서 id만 알고 있으면 삭제, 업데이트를 할 수 있음
>     - 하지만 일반적으로 NoSQL에서는 삭제와 업데이트는 하지 않기 때문에 과연 의미가 있을까?
> - **\_version**:문서 버전
>   - 문서에 대한 CRUD의 결과에 따라서 버전이 올라감
> - **\_score** : 관련성 점수
>   - 문서를 쿼리할 때에 대한 score값(TF-IDF)
> - **\_source** : 문서의 원본 JSON
>

![](../asset/meta_field.PNG)





<br>

## Field의 Data Type

> **tip)**
>
> - 추가되는 필드는 상관이 없음
> - 하지만 필드의 데이터 타입을 바꾸는 것은 불가능

![](../asset/field_datatype.PNG)

- **Text & Keyword**
  - Text에 analyzer를 붙일 수 있음
  - 분석기는 종류가 많음
    - 한국 : Nori, Arirang 분석기 등.
- **Geo-Point**
- **date**
- **object & Array & Nested**

~~~
GET /kibana_sample_data_ecommerce/_stats
GET /kibana_sample_data_ecommerce/_settings
GET /kibana_sample_data_ecommerce/_mapping
~~~



<br>

### RDB vs Document

> Document에 JSON을 전부 때려넣는 것
>
> RDB : 정규화 결과물에 대한 join으로 !!

![](../asset/rdb_doc.PNG)

- Index(Database), Mapping(Schema), Field(Columns)

<br>

## JSON Document NoSQL

> **Keyword** : `Document`, `JSON`, `Mapping`, `indexing`
>
> 문서의 모든 필드가 색인되어 JSON으로 들어감
>
> - 만약 지원하지 않는 데이터타입이 있으면 문제가 생기기 때문에 Mapping을 만들고 접근해야함
>   - ex) location : 위경도의 값을 text로 넣을 수 있음. 만약 색인을 mapping없이 진행하면 index를 지우고 다시 만들어되는 문제가 발생
>

- Default로 **문서의 모든 필드가 색인**되어 JSON 구조로 저장됨
- **Schema Free**를 지원하기 때문에 별도의 mapping이 없어도 JSON문서의 형식으로 데이터를 입력하면 검색이 가능한 형태로 색인 작업이 수행됨

![](../asset/json_document.PNG)

<br>

## Field Data Type

> https://www.elastic.co/guide/en/elasticsearch/reference/7.7/mapping-types.html
>
> https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-types.html

![](../asset/field_datatype.PNG)



<br>

## (중요)String

> **Core datatype**
>
> - **String** : ***text***, ***keyword***
> - Numeric : long, Integer, short, double, float
> - Date
> - Boolean
> - Range : integer_range, float_range, date_range, ip_range

![](../asset/field_datatype_string.PNG)

### Keyword

- 키워드 형태로 사용할 데이터에 적합한 데이터
- **검색 시 필터링**되는 항목
- **정렬이 필요**한 항목
- **(중요)****집계(Aggregation)**해야 하는 항목

### Text(분석기가 파싱하고 검색이 가능)

- 색인시 지정된 **분석기가 데이터를 문자열로 인식하고 분석**함
- **(중요)**기본적으로 **Standard Analyzer**를 사용
- 전문검색(Full Text searching)이 가능
- 텍스트 전체가 토큰화 됨
- 특정 단어 검색 가능

~~~
  
# Data Type - string
# 01. make template
PUT _template/test_template
{
  "index_patterns": [
    "test_index"
  ],
  "mappings": {
    "properties": {
      "book_name01": {
        "type": "keyword"
      },
      "book_name02": {
        "type": "text"
      }
    }
  }
}

# 02. put document
POST test_index/_doc
{
  "book_name01": " 검색엔진 완전 정복",
  "book_name02": " 검색엔진 완전 정복"
}

# 03. search "book_name01"
GET /test_index/_search
{
  "query": {
    "term": {
      "book_name01": "완전"
    }
  }
}

# 04. search "book_name02"
GET /test_index/_search
{
  "query": {
    "term": {
      "book_name02": "완전"
    }
  }
}
~~~





<br>

## Numeric

> https://www.elastic.co/guide/en/elasticsearch/reference/current/number.html
>
> Core datatype
>
> - String : text, keyword
> - **Numeric** : **long**, ***Integer***, short, ***double***, ***float***
> - Date
> - Boolean
> - Range : integer_range, float_range, date_range, ip_range

![](../asset/field_datatype_numeric.PNG)

### 정수형

- 작은 타입을 선택할수록 검색 성능 반영
  - sum, average, min, max 등....
- 어떤 타입이든 저장소엔 영향이 없음
- 실제 값과 담을 수 없는 타입을 선택하면 Error

### 부동소수점

- 타입에 따라 저장소에 영향이 있음



## (중요)Date

> https://www.elastic.co/guide/en/elasticsearch/reference/current/date.html
>
> **Core datatype**
>
> - String : text, keyword
> - Numeric : long, Integer, short, double, float
> - **Date**
> - Boolean
> - Range : integer_range, float_range, date_range, ip_range

- **JSON은 Date타입을 문자열로 처리**
- **날짜는 다양한 format으로 표현 가능 "format"을 지정해야 함**
- 별도로 지정하지 않으면 **"yyyy-MM-ddTHH:mm:ssZ"**
- **내부적으로 UTC의 millis second 단위로 저장된다**.

![](../asset/field_datatype_date.PNG)

~~~
# Data Type - date
# 01. make mapping
PUT my_index_date
{
  "mappings": {
    "properties": {
      "date": {
        "type": "date",
        "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
      }
    }
  }
}

PUT my_index_date
{
  "mappings": {
    "properties": {
      "date": {
        "type": "date",
      }
    }
  }
}



# 02. put document
PUT my_index_date/_doc/1
{ "date": "2015-01-01" } 

PUT my_index_date/_doc/2
{ "date": "2015-01-01T12:10:30Z" } 

PUT my_index_date/_doc/3
{ "date": 1420070400001 } 

# 03. search document
GET my_index_date/_search
{
  "sort": { "date": "asc"} 
}
~~~



## Geo-point

> https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-point.html
>
> - **Geo datatypes**
>   - **Geo-point** : lat/lon
>   - **Geo-shape** : polygons

- 위도, 경도 등 위치 정보를 담은 데이터를 저장
- **(중요)위치 기반 데이터를 색인하고 검색 가능**
- **위치 기반 쿼리가 수행가능**
  - 반경 내 쿼리
  - 위치 기반 집계
  - 위치별 정렬

![](../asset/field_datatype_geopoint.PNG)

~~~
# # Data Type - Geo point
# 01. make mapping
PUT my_index_geo
{
  "mappings": {
    "properties": {
      "location": {
        "type": "geo_point"
      }
    }
  }
}

# 02. save Document
PUT my_index_geo/_doc/1
{
  "text": "Geo-point as an object",
  "location": { 
    "lat": 41.12,
    "lon": -71.34
  }
}

PUT my_index_geo/_doc/2
{
  "text": "Geo-point as a string",
  "location": "41.12,-71.34" 
}

PUT my_index_geo/_doc/3
{
  "text": "Geo-point as a geohash",
  "location": "drm3btev3e86" 
}

PUT my_index_geo/_doc/4
{
  "text": "Geo-point as an array",
  "location": [ -71.34, 41.12 ] 
}

PUT my_index_geo/_doc/5
{
  "text": "Geo-point as a WKT POINT primitive",
  "location" : "POINT (-71.34 41.12)" 
}

# 03. search
GET my_index_geo/_search
{
  "query": {
    "geo_bounding_box": { 
      "location": {
        "top_left": {
          "lat": 42,
          "lon": -72
        },
        "bottom_right": {
          "lat": 40,
          "lon": -74
        }
      }
    }
  }
}
~~~



### Tip)

> Field Data Type
>
> Mapping Parameter
>
> - 색인할 필드의 데이터를 어떻게 저장할지에 대한 다양한 옵션 제공
>
> Meta Field
>
> - ES에서 생성한 문서에서 제공하는 특별한 field. 메타 데이터를 저장하는 특수한 필드로 이를 활용하면 검색시 문서에 다양한 형태로 제어가능

~~~
GET /_cat/templates?v
GET /_cat/indices?v
GET /test_index/_stats
GET /test_index/_settings
GET /test_index/_mapping
~~~







## Template & Mapping API

>- https://www.elastic.co/guide/en/elasticsearch/reference/7.7/indices-templates.html
>- https://www.elastic.co/guide/en/elasticsearch/reference/7.7/mapping.html

![](../asset/template.PNG)

~~~
PUT _template/template_1 # 템플릿 이름
{
  # 적용할 index이름 패턴
  "index_patterns": ["te*", "bar*"],
  # 추가적인 index setting
  "settings": {
    "number_of_shards": 1
  },
  # mapping -> schema
  "mappings": {
    "_source": {
      "enabled": false
    },
    "properties": {
      "host_name": {
        "type": "keyword"
      },
      "created_at": {
        "type": "date",
        "format": "EEE MMM dd HH:mm:ss Z yyyy"
      }
    }
  }
}
~~~





# 2. 분산 저장소 그리고 Elasticsearch

> **샤드의 크기 고려사항**
>
> - 일반적으로 샤드의 크기를 10G ~ 20G가 기본이라고 생각함
> - 로그는 일반적으로 10G미만이라 샤드에 대해 따로 염두하지 않고 사용
>
> **인덱스 패턴 : log-***
>
> - 로그를 하나의 index에 하루 단위로 관리
>   - 인덱스가 매일같이 생성됨(날짜 기준)
>
> **예시)**
>
> - log-20200725 => 샤드 5개
> - log-20200726 => 샤드 10개(데이터가 증가하면 샤드개수를 증가)
>
> - 검색시에 인덱스 패턴으로 모든 인덱스를 검색하는 방식으로 접근!!!!

![](../asset/es1.PNG)

![](../asset/es2.PNG)

<br>





## Distributed, Scalable

>- **Keyword** : `Cluster`, `Node`, `index`, `Shard`, `Replica`
>- https://www.elastic.co/guide/en/elasticsearch/guide/current/_scale_horizontally.html

![](../asset/distributed_scalable.PNG)



<br>

## HA, Fault Tolerance

> - **Keyword** : `Cluster`, `Node`, `index`, `Shard`, `Replica`
> - https://www.elastic.co/guide/en/elasticsearch/guide/current/_coping_with_failure.html

![](../asset/ha_faulttolerance.PNG)



<br>



## REST APIs

> https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html
>
> **Keyword** : `Document`, `JSON`, `REST`
>
> - Elasticsearch는 HTTP 프로토콜로 접근이 가능한 **REST API**를 지원
> - 자원별로 고유 URL로 접근이 가능함
> - HTTP 메서드 **PUT, POST, GET, DELETE** 를 이용해서 자원을 처리

![](../asset/rest_apis.PNG)





## Multi-tenancy

> https://www.elastic.co/kr/blog/found-multi-tenancy
>
> - 하나의 소프트웨어에 여러 사용자가 함께 사용하는 것
> - Elasticsearch의 데이터들은 인덱스(Index)라는 논리적인 집한 단위로 구성
> - 서로 다른 저상소에 분산되어 저장
> - 서로 다른 인덱스들을 별도의 커넥션 없이 하나의 질의로 묶어서 검색하고, 검색 결과들을 하나의 출력으로 도출

![](../asset/multi_tenancy.PNG)





<br>

## Elasticsearch Cluster Architecture

> - https://www.elastic.co/guide/en/kibana/master/tutorial-sample-data.html
>
> **Physical** : cluster, node, shard, replica
>
> **Logical** : index

![](../asset/es_cluster_arch.PNG)



<br>

## Index

> **index 단위로 데이터를 처리**하고 ES가 직접 노드를 분산시켜서 작업을 수행
>
> - 실제로는 각 **Shard**를 가르키고 있는 논리적인 네임스페이스임
> - 샤드는 루씬에서 사용되는 메커니즘인데 *데이터 검색을 위해서 구분되는 최소의 단위 인스턴스*
> - index된 데이터는 *여러 개의 shard로 분리되어서 저장*
> - 즉, 검색의 실상은 루씬이 Shard의 검색을 수행하는 것
>   - segment는 루씬의 영역

![](../asset/index.PNG)

~~~shell
index.number_of_shards: 3
index.number_of_replicas: 1
refresh_interval: 30

# 대용량 처리할 떄 중요한 옵션
# 하나의 노드 당 shard의 갯수를 제한하는 것
# shard가 몰리는 현상이 있음
index.routing.allocation.total_shards_per_node : 2
~~~

**대용량 데이터 처리시에 발생하는 문제점**

- ex) node를 추가 -> 1일 단위로 index가 생성된다는 시나리오
  - 새롭게 프로비전한 노드에 shard가 몰려있을 수 있음
  - 이유 : 기존의 노드에는 이미 샤드들이 많이 존재하기 때문에 새로운 노드에 shard가 몰림(skew발생)

<br>

### Physical & Logical Architecture

> - https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html
> - https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-total-shards.html
> - https://www.elastic.co/kr/blog/how-many-shards-should-i-have-in-my-elasticsearch-cluster

![](../asset/index2.PNG)



<br>

## Cluster & Node

> 마스터 노드는 홀수로 설정하는 것이 좋음
>
> - 실제 트래픽이 많이 몰리지 않기 때문에 초기에는 데이터노드와 마스터 노드를 함께 사용할 수 있음
> - 하지만 데이터의 크기가 커지면 무조건 분리해서 사용
>
> 데이터 노드들의 갯수를 크게 설정하는 것이 좋음
>
> Coordinate, Ingest, ML node 모두 유료임

![](../asset/cluster_node.PNG)

- **Master Nodes** : 클러스터 관리 노드, 클러스터 상태 정보 관리
- **Data nodes** : 데이터가 저장되는 노드, 인덱싱 및 검색 작업 수행, transport 모듈로 통신하기 때문에 http 모듈은 비활성화
- **Ingest nodes** : 데이터 가공 기능 제공
- **Coordinating Nodes** : 검색 및 대향 색인 요청 시 분산 저장된 데이터를 단일 결과 집합화
- **Alerting Nodes** : X-Pack Alerting 실행 노드
- **Machine Learning Nodes** : X-Pack machine learning job을 수행하기 위한 전용 노드



<br>

## Shard & Replica

> ### Shard
>
> - routing동작을 통해 shard에 분산 저장된다
>
> - 내부적인 routing동작을 통해 shard에 분산 저장
>
> ### Replica
>
> - 레플리카는 보통 1로 지정하는게 일반적. 2개로 되면 데이터의 크기가 3배로 증가
>
> - 레플리카가 필요한 이유
>  - 검색 성능(search performance) 향상
>   - 장애복구(fail-over)

![](../asset/shard_replica.PNG)

~~~shell
index.number_of_shards : 3  	# 원본을 3개로 쪼갬
index.number_of_replicas : 1 	# 복사본을 1개 세트
~~~

- 운영시에 하나의 인덱스가 수십테라 이상이 되면 분리해서 사용하는 것을 권장
- 검색시에 shard의 개수가 많고, 크기가 클수록 검색시간이 많이 걸림
- 검색은 모든 replica에서 병렬로 실행가능 -> 검색 성능 수평 확장(scale out) 가능





<br>

## Segment

> 샤드에서 검색 시, 먼저 segment를 검색하여 결과를 조합한 최종 결과를 결과로 리턴함
>
> **루씬의 영역**
>
> - segment에 내려가기 전에 메모리에 저장
> - 샤드 내부에 segment가 있음
> - 세그먼트 내부에 별도의 index가 있음.
> - tip) 루씬은 분산처리 X

![](../asset/segment0.PNG)

![](../asset/segment.PNG)

- ES에 데이터를 저장하면, 엘라스틱서치는 이것을
  메모리에 모아두고 새로운 세그먼트를 디스크에
  기록하여 검색을 refresh
- 그러나 세그먼트가 fsync되지 않았으므로 여전히
  데이터 손실의 위험이 남아있다. ES는 세그먼트를
  fsync하는 "flush"를 주기적으로 진행,􀀁불필요한
  트랜젝션 로그를 비움



<br>

## Hot & Warm Architecture

> - https://www.elastic.co/kr/blog/hot-warm-architecture-in-elasticsearch-5-x
> - https://www.elastic.co/kr/blog/implementing-hot-warm-cold-in-elasticsearch-with-index-lifecycle-management

![](../asset/hot-warm.PNG)

> Elasticsearch가 역할을 나눔(template으로 라벨 설정)
>
> - Hot 라벨 노드 : 오늘자 index shard들을 hot 라벨 노드에 분배, CPU를 많이 먹음
> - Warm 라벨 노드 : 최신에 상대적으로 먼 애들을 warm 라벨 노드에 분배(rellocate), Memory를 많이 먹음
>
> Hot 노드 : 새로운 문서가 작성된 색인을 지원
>
> Warm 노드 : 빈번하게 질의 될 가능성이 없는 일기 전용 색인을 처리하기 위함. 검색 query가 빈번하지 않지만, 양이 많은 read-only indice를 담당

search : 노드(CPU를 만이 사용)

index : 노드()



<br>

<br>

# 3. 분석 검색엔진 그리고 Elasticsearch

> Text라고 붙는 field에 분석기를 다양하게 달 수 있음

![](../asset/es4.PNG)

![](../asset/es3.PNG)

<br>



## Near Real-Time(NRT)

> **Keyword** : `Document`, `Segment`, `Inverted Index`, `Indexing`, `Refresh`, `
>
> - https://www.elastic.co/guide/en/elasticsearch/guide/current/near-real-time.html

![](../asset/nrt.PNG)

<br>



## Full Text Searching Engine

> - **Keyword** : `Document`, `Segment`, `Inverted Index`, `Indexing`, `Token`
> - https://www.elastic.co/kr/blog/found-elasticsearch-from-the-bottom-up

![](../asset/full_text_search_engine.PNG)

<br>







## Analyzer

> **Analyzer는 역색인을 만들어냄**
>
> - 토큰을 짜르고 분석을 수해하는 것은 루씬이 하는 것
> - analysis는 Full text를 term으로 변환하는 과정

- ES는 루씬을 기반으로 구축된 Text기반 검색엔진

- 루씬은 내부적으로 다양한 분석기르 제공하는데 ES는 루씬이 제공하는 분석기를 그대로 사용

  - **Analyzer --> 역색인(Inverted Index)를 만들어냄**

- Analysis는 **Full text**를 **용어(term)**으로 변환하는 과정

  - 어떤 **분석기(analyzer)**를 사용하는지에 따라 FOO BAR, Foo-Bar, foo, bar 등이 foo, bar 등의 용어로 변환
- 이런 **용어(term)**들은 실제 index에 저장되는 것들
  - Full text query(term query와 다름) foO:bAR는 용어(term) foo, bar로 분석되고 index에 저장됨

<br>

## Analyzer 구조

> Character filter -> TokenizerToken Filter
>

![](../asset/analyzer.PNG)

- Text field가 저장될 때 데이터에서 검색어 토큰을 저장하기 위해 여러 단계를 거침
  - 이 전체 과정을 **Text Analysis**
  - 이 과정을 처리하는 기능이 **Analyzer**
- Analyzer = Character Filter + Tokenizer + Token Filter

<br>

### Analyzer 예시

![](../asset/analyzer2.PNG)

- 문장을 분석하기 전에 입력 텍스트에 대해 특정 단어를 변경하거나, HTML태그를 제거하는 역할을 하는 필터. 텍스트를 개별 토큰화 하기 전의 전처리 과정. 0 ~ N개
- 분석기에서 1개만을 사용. 텍스트를 어떻게 나눌 것인지를 정의. 한글 -> "한글 형태소 분석기의 Tokenizer"
- 토큰화된 단어를 하나씩 필터링해서 사용자가 원하는 토큰으로 변환. 불필요한 단어 삭제, 동의어 사전. 0 ~ N개

~~~
POST _analyze
{
  "analyzer": "standard",
  "text": "우리나라 좋은나라, 대한민국 화이팅"
}
~~~





<br>

## Inverted Index

> https://www.elastic.co/kr/blog/found-elasticsearch-from-the-bottom-up

![](../asset/inverted_index.PNG)

- **Index**
  - key-value 구조를 갖는 테이블
  - key-value 구조를 갖는 테이블은 HashMap이라는 자료구조와 연관이 있음. 특히, 대용량 데이터를 빠르게 탐색 할 대에 유요한 자료구조

- **Forward index**
  - index라고 말하는 것은 forward index를 말함
  - 어떠한 주어진 Document들로 index를 생성하면 Document가 Key가 되고 Document에 존재하는 Word들이 Value에 해당함

- **Inverted index**
  - forward index가 아닌 inverted index는 Word가 Key가 됨. 그리고 Word가 존재하는 Document들이 value가 됨. 
    - toy라는 key로 검색해 toy라는 단어가 있는 Document들을 찾을 수 있음









<br>

<br>

## Mapping API

~~~
# 01. index create & delete
PUT test_index
DELETE test_index

# 02. check index information
GET /_cat/templates?v 
GET /_cat/indices?v 
GET /test_index/_stats 
GET /test_index/_settings 
GET /test_index/_mapping

# 03. make mapping
PUT _template/test_template_mapping_api
{
  "index_patterns": [
    "test_index_mapping_api"
  ],
  "mappings": {
    "properties": {
      "book_name": {
        "type": "keyword"
      },
      "description": {
        "type": "text"
      },
      "price": {
        "type": "integer"
      },
      "location": {
        "type": "geo_point"
      },
      "order_time": {
        "type": "date"
      }
    }
  }
}

# 04. save document
POST test_index_mapping_api/_doc
{
  "book_name": "검색엔진 완전 정복",
  "description": "이 책은 검색엔진에 관현 책이며, 완전 정복 책으로 베스트 셀러 10위이다.",
  "price": 25000,
  "location": "41.12,-71.34",
  "order_time": "2020-07-25T12:10:30Z"
}

# 05. search
GET test_index_mapping_api/_search
{
  "query": {
    "match_all": {}
  }
}

GET test_index_mapping_api/_search
{
  "query": {
    "term": {
      "book_name": "완전"
    }
  }
}

GET test_index_mapping_api/_search
{
  "query": {
    "term": {
      "description": "완전"
    }
  }
}
~~~



<br>

## Document API

> - **bulk API** : https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html
>   - multiple indexing 또는 delete 작업을 단일 API 호출로 수행. 오버헤드를 줄이고 빠른 indexing 스피드를 가능하게 함
> - **reindex API** : https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html
>   - 하나의 index에서 다른 index로 document를 copy
> - **analyzer API** : https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer.html
>   - **Korean**
>     - https://www.elastic.co/kr/blog/how-to-search-ch-jp-kr-part-1
>     - https://www.elastic.co/kr/blog/how-to-search-ch-jp-kr-part-2

~~~
# Document API
POST /books/_doc/1
{
  "title": "ElasticSearch Guide",
  "author": "Kim",
  "started": "2014-03-14",
  "pages": 250
}

GET /books/_doc/1

PUT /books/_doc/1
{ 
  "title" : "ElasticSearch Guide", 
  "author" : ["Kim", "Lee"], 
  "started" : "2014-03-14", 
  "pages" : 250 
}

DELETE /books/_doc/1


# Bulk API
POST _bulk
{ "index" : { "_index" : "books", "_id": 1 } }
{ "title" : "ElasticSearch Guide", "author" : ["Kim", "Lee", "Park"], "started" : "2020-06-14", "pages" : 250 }
{ "index" : { "_index" : "books", "_id": 2 } }
{ "title" : "Test Book", "author" : ["Park"], "started" : "2020-06-14", "pages" : 100 }

# Reindex API
# 01. save Document
POST /log-2020-07-21/_doc/1
{  
    "projectName" : "카카오",
    "logType": "web-log",
    "logSource": "장바구니",
    "logTime":"2020-07-21T06:07:49",
    "host": "host01",
    "body": "[2020-07-21T06:07:49.332Z] GET /beats HTTP/1.1 200 7691 - Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1"
}

POST /log-2020-07-21/_doc/2
{  
    "projectName" : "카카오",
    "logType": "web-log",
    "logSource": "장바구니2",
    "logTime":"2020-07-21T08:07:49",
    "host": "host02",
    "body": "[2020-07-21T06:07:49.332Z] GET /beats HTTP/1.1 200 7691 - Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1"
}

GET log-2020-07-21/_search


# 02. make template
PUT _template/test_template_reindex
{
  "index_patterns": [
    "log-2020-07-21-reindex"
  ],
  "mappings": {
    "properties": {
      "projectName": {
        "type": "keyword"
      },
      "logType": {
        "type": "keyword"
      },
      "logSource": {
        "type": "keyword"
      },
      "logTime": {
        "type": "date"
      },
      "host": {
        "type": "keyword"
      },
      "body": {
        "type": "text"
      }
    }
  }
}


# 03. copy index
POST _reindex
{
  "source": {
    "index": "log-2020-07-21"
  },
  "dest": {
    "index": "log-2020-07-21-reindex"
  }
}

POST _reindex
{
  "source": {
    "index": "log-2020-07-21",
    "query": {
      "term": {
        "host": "host01"
      }
    }
  },
  "dest": {
    "index": "log-2020-07-21-reindex02"
  }
}


# 04. check index & search
GET log-2020-07-21-reindex/_search
GET log-2020-07-21-reindex/_mapping


# log-2020-07-21, log-2020-07-21-reindex 인덱스에 한번에 검색 요청
GET /log-2020-07-*/_search
~~~

<br>



## Search API

> **참조**
>
> - https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html
> - https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html
>
> **Match All Query** : `match_all`
>
> - Index의 모든 Documents조회. 가장 단순하고 일반적인 쿼리
>
> **Full Text(Match) Query** : `match`, `query-string`
>
> - 텍스트를 분석기를 통해 분석후 쿼리로 검색을 수행
> - `match` : 검색어와 부분적으로 일치하는 value를 가진 Document 조회
> - `query-string`: 연산자를 중심으로 텍스트를 분할하여 쿼리를 분석
>
> **Term Level Query** : `exists`, `fuzzy`, `range`, `term`, `prefix`, `wildcard`
>
> - 별도의 분석작업을 통하지 않고 입력된 텍스트가 존재하는 문서를 찾음
> - 일반적으로 숫자, keyword, 날짜 데이터를 쿼리하는데 사용
> - `term`: 검색어와 정확히 일치하는 value를 가진 Document 조회
> - `terms`: 검색어 중 적어도 1개와 정확히 일치하는 Document 조회
> - `range`: 특정 Numeric Field가 임의의 범위 내에 있는 Documents 조회
> - `prefix`: 특정 접두어로 시작하는 Document 조회
> - `wildcard` : Wildcard Expression을 만족하는 Documents 조회(\* / ?)
> - `fuzzy`: 검색어와 유사한 Documents 조회
>
> **Compound Query** : `bool`
>
> - AND, OR, NOT 연산자를 이용해 쿼리를 조합하여 사용
> - https://www.elastic.co/guide/en/elasticsearch/reference/6.2/query-dsl-bool-query.html

![](../asset/compoundquery_bool.PNG)

~~~
# 01. Match All Query
# 모든 Documents 조회
GET /test_data/_search
{
  "query": {
    "match_all": {}
  }
}

# Document 정렬
GET /test_data/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "customer_age": {
        "order": "desc"
      }
    }
  ]
}

# Document 검색 size 조정
GET /test_data/_search
{
  "query": {
    "match_all": {}
  },
  "from": 0, 
  "size": 100
}


# 02. Full Text Query
# 02. Full Text Query -  match
GET /test_data/_search
{
  "query": {
    "match": {
      "구매사이트": "G마켓"
    }
  }
}

# 02. Full Text Query - query-string
GET /test_data/_search
{
  "query": {
    "query_string": {
      "query": "customer_age : [10 TO 25]"
    }
  }
}

# 03. Term Level Query
# term
GET /test_data/_search
{
  "query": {
    "term": {
      "product_분류": "셔츠"
    }
  }
}
# terms
GET /test_data/_search
{
  "query": {
    "terms": {
      "product_분류": ["셔츠", "스웨터"]
    }
  }
}

# range
GET /test_data/_search
{
  "query": {
    "range": {
      "order_주문시간": {
        "gte": "2020-06-15",
        "lte": "2020-06-20"
      }
    }
  }
}

# prefix
GET /test_data/_search
{
  "query": {
    "prefix": {
      "customer_address": "경상"
    }
  }
}

# wildcard - *, ?
GET /test_data/_search
{
  "query": {
    "wildcard": {
      "customer_address": "경*도"
    }
  }
}

GET /test_data/_search
{
  "query": {
    "wildcard": {
      "customer_address": "경?도"
    }
  }
}

# fuzzy
GET /test_data/_search
{
  "query": {
    "fuzzy": {
      "customer_address": "경상북남"
    }
  }
}

# 04. Compound Query
# customer = 서울특별시 AND (구매사이트 = 11로 시작 OR customer_age < 30)
GET /shopping/_search
{
  "query": {
    "bool": {
      "must": [
        { "term": { "customer_address": "서울특별시" } }
      ], 
      "should": [
        { "prefix": { "구매사이트" : "11" } },
        { "range": { "customer_age": { "lt": 30 } } }
      ],
      "minimum_should_match": 1
    }
  }
}
~~~



<br>

## Analyzer API

~~~
POST _analyze
{
  "analyzer": "standard",
  "text": "우리나라 좋은나라, 대한민국 화이팅"
}
~~~



